---
layout: page
title: Home
---
<section class="c-archives">
<h2 class="c-archives__year">About</h2>
<ul class="c-archives__list">
    <li class="c-archives__item">
    <p style="color: black;">
        Hi, I'm Yuwei, a CS PhD student at National University
        of Singapore.  My research is motivated by deploying robots into open-ended, 
        unstructured environments for general and timely assistance. In particular, 
        I'm interested in scalable, adaptive, and reusable robot skill acquisition 
        through low-resource imitation learning and reinforcement learning. 
        <br>
        <br>
        Previously, I worked on applied research in learning for robot manipulation at 
        <a href="https://www.youtube.com/watch?v=2Bh61aY8ncg">Dyson robotics
            research</a>, where I was fortunate to have worked with a team of passionate 
        roboticists and <a href="https://profiles.imperial.ac.uk/e.johns">Dr.
            Edward Johns</a>. I have also worked on a series of 
        interesting real-life deep learning solutions as an R&D engineer at 
        <a href="https://research.sg.panasonic.com/">Panasonic R&D Center
            Singapore</a>. Before that, 
        I obtained my bachelor's degree from Nanyang Technological University,
        with my thesis supervised by <a href="https://microelectronics.tudelft.nl/People/bio.php?id=744">Prof Justin Dauwels</a>.
    </p>
    <li>
</ul>
<br> 
<br>

<h2 class="c-archives__year">Publication</h2>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Learning Reward for Robot Skills Using Large Language Models via Self-Alignment<br></p>
        <p style="font-size:75%;"><b>Yuwei Zeng</b>, Yao Mu, Lin Shao<br>ICML 2024<br><a href="https://openreview.net/pdf?id=Z19JQ6WFtJ">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/rewardselfalign">Project</a><span class="v-separate"></span><a href="https://github.com/friolero/self_aligned_reward_learning">Code</a></p>
    <li>
    <br>
    <li class="c-archives__item">
        <p style="color: black;"> Diff-lfd: Contact-Aware Model-Based Learning from Visual Demonstration for Robotic Manipulation via Differentiable Physics-Based Simulation and Rendering<br></p>
        <p style="font-size:75%;">Xinghao Zhu, JingHan Ke, Zhixuan Xu, Zhixin Sun, Bizhe Bai, Jun Lv, Qingtao Liu, <b>Yuwei Zeng</b>, Qi Ye, Cewu Lu, Masayoshi Tomizuka, Lin Shao<br>CoRL 2023 (Oral)<br><a href="https://proceedings.mlr.press/v229/zhu23a/zhu23a.pdf">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/diff-lfd">Project</a></p>
    <li>
    <br>
    <li class="c-archives__item">
        <p style="color: black;">Learning Reward for Physical Skills using Large Language Model<br></p>
        <p style="font-size: 75%;"><b>Yuwei Zeng</b>, Yiqing Xu<br>CoRL 2023 Workshop LangRob<br><a href="https://openreview.net/forum?id=KlK1i2oBpI">Paper</a><span class="v-separate"></span><a href="https://drive.google.com/file/d/143ss4LUJzIQRhssbANCKOgbQAfXhgKz7/view">Poster</a><span class="v-separate"></span><a href="https://github.com/friolero/self_aligned_reward_learning">Code</a></p>
    <li>
    <br>
    <li class="c-archives__item">
        <p style="color: black;">ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment<br></p>
        <p style="font-size:75%;">Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, <b>Yuwei Zeng</b>, Jun Lv, Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, Lin Shao<br>ICCV 2023<br><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ClothesNet_An_Information-Rich_3D_Garment_Model_Repository_with_Simulated_Clothes_ICCV_2023_paper.pdf">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/clothesnet">Project</a><span class="v-separate"></span><a href="https://docs.google.com/forms/d/e/1FAIpQLSdE-cUxWSzvC-D99RqkIHI9yLHjvT_5QygszjfqxnB6vIt8vw/viewform">Dataset</a></p>
    <li>
</ul>
<br>
<br>
<h2 class="c-archives__year">Projects</h2>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Template-Based Single-View Cloth Shape Estimation with Data-Driven Deformation Prediction and Differentiable Rendering<br></p>
        <p style="font-size:75%;">
            <a href="https://drive.google.com/file/d/1yD12kPceUxEfWFQzrTz6WXyP5n_IpPAE/view?usp=drive_link">PDF</a><span class="v-separate"></span><a href="https://github.com/friolero/cs6244_cloth_shape_estimation">Code</a></p>
    <li>
</ul>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Object Grasping and Lifting with KG-3 Gripper from Single Human Demonstration<br></p>
        <p  style="font-size:75%;">
            <a href="https://drive.google.com/file/d/17Ult7o3O9O-b8rtlBymSOf7oyac_0wkz/view?usp=sharing">PDF</a><span class="v-separate"></span><a href="https://github.com/friolero/tulip/blob/main/examples/robots/transfer_grab_demo.py">Code</a></p>
    <li>
</ul>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Sampling, Optimization and Diffusion-Based Motion Planning for 2D Navigation<br></p>
        <p style="font-size:75%;">
            <a href="https://drive.google.com/file/d/15pugl4e622GVE814Wak4dJgszsedWJoi/view?usp=drive_link">PDF</a><span class="v-separate"></span><a href="https://github.com/friolero/cs5478_path_planning/tree/main">Code</a></p>
    <li>
</ul>

</section>
