---
layout: page
title: Home
---
<section class="c-archives">
<h2 class="c-archives__year">Publication</h2>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Learning Reward for Robot Skills Using Large
        Language Models via Self-Alignment<br></p>
        <p style="font-size:75%;"><b>Yuwei Zeng</b>, Yao Mu, Lin Shao<br>ICML 2024<br><a href="https://openreview.net/pdf?id=Z19JQ6WFtJ">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/rewardselfalign">Project</a><span class="v-separate"></span><a href="https://github.com/friolero/self_aligned_reward_learning">Code</a></p>
    <li>
    <li class="c-archives__item">
        <p style="color: black;"> Diff-lfd: Contact-aware model-based learning from visual demonstration for robotic manipulation via differentiable physics-based simulation and rendering<br></p>
        <p style="font-size:75%;">Xinghao Zhu, JingHan Ke, Zhixuan Xu, Zhixin Sun, Bizhe Bai, Jun Lv, Qingtao Liu, <b>Yuwei Zeng</b>, Qi Ye, Cewu Lu, Masayoshi Tomizuka, Lin Shao<br>CoRL 2023 (Oral)<br><a href="https://proceedings.mlr.press/v229/zhu23a/zhu23a.pdf">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/diff-lfd">Project</a><br>
    <li>
    <li class="c-archives__item">
        <p style="color: black;">Learning Reward for Physical Skills using Large Language Model<br></p>
        <p style="font-size: 75%;"><b>Yuwei Zeng</b>, Yiqing Xu<br>CoRL 2023 Workshop LangRob<br><a href="https://openreview.net/forum?id=KlK1i2oBpI">Paper</a><span class="v-separate"></span><a href="https://drive.google.com/file/d/143ss4LUJzIQRhssbANCKOgbQAfXhgKz7/view">Poster</a><span class="v-separate"></span><a href="https://github.com/friolero/self_aligned_reward_learning">Code</a></p>
    <li>
    <li class="c-archives__item">
        <p style="color: black;">ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment<br></p>
        <p style="font-size:75%;">Bingyang Zhou, Haoyu Zhou, Tianhai Liang, Qiaojun Yu, Siheng Zhao, <b>Yuwei Zeng</b>, Jun Lv, Siyuan Luo, Qiancai Wang, Xinyuan Yu, Haonan Chen, Cewu Lu, Lin Shao<br>ICCV 2023<br><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ClothesNet_An_Information-Rich_3D_Garment_Model_Repository_with_Simulated_Clothes_ICCV_2023_paper.pdf">Paper</a><span class="v-separate"></span><a href="https://sites.google.com/view/clothesnet">Project</a><span class="v-separate"></span><a href="https://docs.google.com/forms/d/e/1FAIpQLSdE-cUxWSzvC-D99RqkIHI9yLHjvT_5QygszjfqxnB6vIt8vw/viewform">Dataset</a></p>
    <li>
</ul>
<br>
<br>
<h2 class="c-archives__year">Projects</h2>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Template-based Single-view Cloth Shape Estimation with Data-driven Deformation Prediction and Differentiable Rendering<br></p> <a
            href="https://github.com/friolero/cs6244_cloth_shape_estimation/blob/main/final_report.pdf">PDF</a><span
            class="v-separate"></span><a href="https://github.com/friolero/cs6244_cloth_shape_estimation">Code</a></p>
    <li>
</ul>
<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Object Grasping and Lifting with KG-3 Gripper from Single Human Demonstration<br><a
            href="https://drive.google.com/file/d/17Ult7o3O9O-b8rtlBymSOf7oyac_0wkz/view?usp=sharing">PDF</a><span class="v-separate"></span><a
            href="https://github.com/friolero/tulip/blob/main/examples/robots/transfer_grab_demo.py">Code</a>
    <li>
</ul>

<ul class="c-archives__list">
    <li class="c-archives__item">
        <p style="color: black;">Sampling, Optimization and Learning-based Motion Planning for 2D Navigation<br><a
            href="https://github.com/friolero/cs5478_path_planning/blob/main/CS5478_final_report.pdf">PDF</a><span
            class="v-separate"></span><a
            href="https://github.com/friolero/cs5478_path_planning/tree/main">Code</a>
    <li>
</ul>

</section>
